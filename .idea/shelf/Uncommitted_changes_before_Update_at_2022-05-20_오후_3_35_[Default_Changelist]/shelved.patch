Index: preprocess.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/preprocess.py b/preprocess.py
--- a/preprocess.py	(revision 37f4953628acd828d487f22335ffc5f75ead2b91)
+++ b/preprocess.py	(date 1653024959605)
@@ -1,0 +1,50 @@
+import os
+import torch
+from torch.utils.data import DataLoader, Dataset
+import pandas as pd
+
+
+
+import torch.nn
+
+class DementiaDataset(Dataset):
+    def __init__(self):
+        super(DementiaDataset, self).__init__()
+        self.base_path = os.path.join(os.getcwd(), 'dataset')
+        self.control_path = os.path.join(self.base_path, 'control')
+        self.dementia_path = os.path.join(self.base_path, 'dementia')
+
+        self.control_files = os.listdir(self.control_path)
+        self.dementia_files = os.listdir(self.dementia_path)
+        self.dataset = self.control_files + self.dementia_files
+
+
+    def __len__(self):
+        return len(self.control_files) + len(self.dementia_files)
+
+    def __getitem__(self, idx):
+        cutline = len(self.control_files)
+
+        if idx <= cutline:
+            file_path = os.path.join(self.control_path, self.dataset[idx])
+            label = 0
+        else:
+            file_path = os.path.join(self.dementia_path, self.dataset[idx])
+            label = 1
+
+        file = pd.read_csv(file_path, delimiter='\n')
+
+        return file, label
+
+
+class Preprocess:
+    def __init__(self):
+
+        self.max_seq_len = 100          # 언급 없음
+
+        self.dementia = []
+        self.control = []
+
+    def loader(self):
+
+    def padding(self):
Index: model.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import torch\r\nimport torch.nn as nn\r\n\r\n# 1-dimensional CNN\r\nclass CNN(nn.Module):\r\n    def __init__(self, max_seq_len, dropout_rate=0, vocab_size=1751, embedding_dim=100):\r\n        super(CNN, self).__init__()\r\n        self.max_seq_len = max_seq_len              # sentence length\r\n        self.embedding_dim = embedding_dim          # GloVe, Word2Vec 기준으로 범위 잡기\r\n        self.dropout_rate = dropout_rate\r\n\r\n        self.embed = nn.Embedding(vocab_size, embedding_dim)\r\n        # self.multi_size_conv = [nn.Conv1d(in_channels=1, out_channels=1, kernel_size=(kernel_size, embedding_dim), stride=1) for kernel_size in range(1, 6)]     # first conv. layer\r\n        # self.uni_size_conv = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=(3, embedding_dim), stride=1)\r\n        # self.cnn = nn.ModuleList([self.multi_size_conv, self.uni_size_conv, self.uni_size_conv])\r\n\r\n        self.relu = nn.ReLU()\r\n\r\n        self.conv1 = nn.ModuleList([nn.Conv1d(in_channels=self.embedding_dim, out_channels=128,\r\n                                                    kernel_size=kernel) for kernel in range(1, 6)])       # First CNN layers\r\n        self.max1d = None\r\n\r\n    def max_pooling(self, input, kernel, stride):\r\n        self.max1d = nn.MaxPool1d(kernel, stride)\r\n        ret = self.max1d(input)\r\n\r\n        return ret\r\n\r\n    def conv(self, input, in_channel, kernel, out_channel=128):\r\n        layer = nn.Conv1d(in_channels=in_channel, out_channels=out_channel, kernel_size=kernel)\r\n        ret = layer(input)\r\n\r\n        return ret\r\n\r\n    def forward(self, input):           # data loader 어떻게 사용하는지 추가하기\r\n\r\n        input = input.permute(0, 2, 1)\r\n\r\n        x = []\r\n        for cnn in self.conv1:\r\n            feature = cnn(input)          # Embedded Input\r\n            feature = self.relu(feature)\r\n            feature = self.max_pooling(feature, kernel=2, stride=2)\r\n            x.append(feature)\r\n\r\n        x = torch.cat(x, dim=2)\r\n        x = self.conv(x, in_channel=x.size(dim=1), kernel=3)      # 논문 '그림 상'으로는 128이 in channel이므로 reshape 안함\r\n        x = self.relu(x)\r\n        x = self.max_pooling(x, kernel=2, stride=2)\r\n\r\n        x = self.conv(x, in_channel=x.size(dim=1), kernel=3)\r\n        x = self.relu(x)\r\n        x = self.max_pooling(x, kernel=2, stride=2)\r\n\r\n        return x\r\n    #\r\n    # def call(self):\r\n    #     result = self.forward(input)\r\n    #\r\n    #     return result\r\n\r\n\r\nclass BiRNN(nn.Module):\r\n    def __init__(self, model, max_seq_len, embedding_dim=100):\r\n        super(BiRNN, self).__init__()\r\n        self.embedding_dim = embedding_dim\r\n        self.hidden_size = 100                # 논문에 언급 없음\r\n        self.hidden_units = 128\r\n\r\n        self.bi_gru = nn.GRU(input_size=self.embedding_dim, hidden_size=self.hidden_size,\r\n                             num_layers=self.hidden_units, batch_first=True, bidirectional=True)\r\n\r\n        self.bi_lstm = nn.LSTM(input_size=self.embedding_dim, hidden_size=self.hidden_size,\r\n                               num_layers=self.hidden_units, batch_first=True, bidirectional=True)\r\n\r\n    def forward(self, x):\r\n        output, hidden = self.bi_gru(x)\r\n\r\n        return output, hidden\r\n\r\n\r\nclass Attention(nn.Module):         # feed-forward attention\r\n    def __init__(self, d, t):\r\n        super(Attention, self).__init__()\r\n        self.d = d             # CNN: the number of filters = 128  /  Bi LSTM: dim of the contextual word embedding = 128\r\n        self.T = t             # length of generated feature map   /  Bi LSTM: the number of hidden states\r\n\r\n        self.tanh = nn.Tanh()\r\n        self.softmax = nn.Softmax()\r\n\r\n        self.feedforward = nn.Linear(in_features=self.d, out_features=1)\r\n\r\n    def forward(self, x):\r\n        x = torch.transpose(x, -1, -2)\r\n\r\n        m = self.feedforward(x)\r\n        m = self.tanh(m)\r\n        alpha = self.softmax(m)\r\n        r = torch.bmm(torch.transpose(x, -1, -2), alpha)\r\n\r\n        return r\r\n\r\n# Test run\r\nif __name__ == \"__main__\":\r\n\r\n    vocab_size = 1751   # 우선 논문과 동일하게 설정\r\n    embedding_dim = 100    # 100-GloVe 기준\r\n\r\n    embed = nn.Embedding(vocab_size, embedding_dim)
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/model.py b/model.py
--- a/model.py	(revision 37f4953628acd828d487f22335ffc5f75ead2b91)
+++ b/model.py	(date 1653019101639)
@@ -9,7 +9,7 @@
         self.embedding_dim = embedding_dim          # GloVe, Word2Vec 기준으로 범위 잡기
         self.dropout_rate = dropout_rate
 
-        self.embed = nn.Embedding(vocab_size, embedding_dim)
+        # self.embbed = nn.Embedding(vocab_size, embedding_dim)
         # self.multi_size_conv = [nn.Conv1d(in_channels=1, out_channels=1, kernel_size=(kernel_size, embedding_dim), stride=1) for kernel_size in range(1, 6)]     # first conv. layer
         # self.uni_size_conv = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=(3, embedding_dim), stride=1)
         # self.cnn = nn.ModuleList([self.multi_size_conv, self.uni_size_conv, self.uni_size_conv])
@@ -53,16 +53,17 @@
         x = self.max_pooling(x, kernel=2, stride=2)
 
         return x
-    #
-    # def call(self):
-    #     result = self.forward(input)
-    #
-    #     return result
+
+    def call(self):
+        ret = self.forward()
+
+        return ret
 
 
 class BiRNN(nn.Module):
     def __init__(self, model, max_seq_len, embedding_dim=100):
         super(BiRNN, self).__init__()
+        self.model = model
         self.embedding_dim = embedding_dim
         self.hidden_size = 100                # 논문에 언급 없음
         self.hidden_units = 128
@@ -74,10 +75,18 @@
                                num_layers=self.hidden_units, batch_first=True, bidirectional=True)
 
     def forward(self, x):
-        output, hidden = self.bi_gru(x)
+        if self.model == "gru":
+            output, hidden = self.bi_gru(x)
+        elif self.model == "lstm":
+            output, hidden = self.bi_lstm(x)
 
         return output, hidden
 
+    def call(self):
+        ret = self.forward()
+
+        return ret
+
 
 class Attention(nn.Module):         # feed-forward attention
     def __init__(self, d, t):
@@ -100,6 +109,11 @@
 
         return r
 
+    def call(self):
+        ret = self.fowrad()
+
+        return ret
+
 # Test run
 if __name__ == "__main__":
 
Index: train.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/train.py b/train.py
--- a/train.py	(revision 37f4953628acd828d487f22335ffc5f75ead2b91)
+++ b/train.py	(date 1653020448294)
@@ -1,0 +1,37 @@
+import torch
+import torch.nn as nn
+import os
+import sys
+import embedding
+import model
+
+
+if __name__ == "__main__":
+
+    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
+
+
+    lr = 1e-3
+    batch_size = 100        # 임의 지정. 바꾸기.
+    dropout_rate = 0.3      # 논문 언급 없음.
+    weight_decay = 2e-5
+    embedding_size = 100    # 여러 차원으로 실험해보기
+    max_seq_length = 100    # 논문 언급 없음.
+    seed = 1024
+    num_classes = 2
+
+    # 10-fold cross validation 적용
+
+    # Embedding
+
+    """
+    CNN + Attention
+    """
+
+    """
+    BiGRU + Attention
+    """
+
+    """
+    concat + Softmax
+    """
\ No newline at end of file
